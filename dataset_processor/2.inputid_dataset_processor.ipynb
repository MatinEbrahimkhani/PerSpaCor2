{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T13:20:34.996649Z",
     "start_time": "2024-06-19T13:20:34.992770Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict,Dataset,concatenate_datasets\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:20:36.327470Z",
     "start_time": "2024-06-19T13:20:34.998195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labeled_dataset = DatasetDict.load_from_disk(\"./../_data/datasets/labeled/all_sliced\")\n",
    "pretrained_model = \"bert-base-multilingual-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ],
   "id": "3b466cd08348c00c",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:38:12.619085Z",
     "start_time": "2024-06-19T13:20:36.328451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(sent_tok):\n",
    "    # Tokenize the input text\n",
    "    tokens = sent_tok[\"tokens\"]\n",
    "    labels =  sent_tok[\"labels\"]\n",
    "    labels.insert(0, 0)  # Adding CLS token label at the beginning of each sequence\n",
    "    labels.append(0)\n",
    "    try:\n",
    "        tokenized_sentence = [101] + [tokenizer.encode(tok)[1] for tok in tokens] + [102]\n",
    "        sent_tok[\"input_ids\"] = tokenized_sentence\n",
    "        sent_tok[\"labels\"] = labels\n",
    "    except:\n",
    "        print(len(tokens))\n",
    "    return sent_tok\n",
    "inputid_dataset = labeled_dataset.map(tokenize_function, load_from_cache_file=False) \n",
    "inputid_dataset"
   ],
   "id": "4663f0addc453596",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:38:13.754151Z",
     "start_time": "2024-06-19T13:38:12.620246Z"
    }
   },
   "cell_type": "code",
   "source": "inputid_dataset.save_to_disk(\"./../_data/datasets/inputid/bert-mult-uncased\")",
   "id": "37b8d20c8a3ac41f",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:38:13.776091Z",
     "start_time": "2024-06-19T13:38:13.756240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an empty dictionary with specified columns\n",
    "empty_data = { \n",
    "    \"input_ids\": [[1]],  # Replace with your actual input IDs\n",
    "    \"labels\": [[0]],    # Replace with your actual labels\n",
    "    \"attention_mask\": [[1]],  # Replace with your actual attention mask\n",
    "}\n",
    "batched_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(empty_data),\n",
    "    \"validation\": Dataset.from_dict(empty_data),\n",
    "    \"test\": Dataset.from_dict(empty_data)\n",
    "})\n"
   ],
   "id": "f6e9a8ad59786b65",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:38:13.782516Z",
     "start_time": "2024-06-19T13:38:13.777092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from _utils.dataset_util import flatten_2d_list\n",
    "def chunck_pad_map(sents_batch,dataset_name):\n",
    "    \n",
    "    f_tokens = flatten_2d_list(sents_batch[\"tokens\"])\n",
    "    f_labels = flatten_2d_list(sents_batch[\"labels\"])\n",
    "    f_input_ids = flatten_2d_list(sents_batch[\"input_ids\"])\n",
    "    # print(f_tokens)\n",
    "    # print(len(f_input_ids))\n",
    "    # print(len(f_labels))\n",
    "    # # print(sents_batch[\"tokens\"])\n",
    "    # # print(len(sents_batch[\"tokens\"]))\n",
    "    # raise EOFError\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    chunk_size =512\n",
    "    tok_pad=0 \n",
    "    label_pad=-100\n",
    "    attention_pad=0\n",
    "    # Create chunks\n",
    "    # print(len(f_input_ids)// chunk_size)\n",
    "    for i in range(0, len(f_input_ids), chunk_size):  # We subtract 2 to account for special tokens\n",
    "        chunked_tokens = f_input_ids[i:i + chunk_size]\n",
    "        chunk_label_ids = f_labels[i:i + chunk_size]\n",
    "        chunk_attention_mask = [1] * len(chunked_tokens)\n",
    "        \n",
    "        # print(len(chunked_tokens))\n",
    "        \n",
    "        while True and len(chunked_tokens) < chunk_size:\n",
    "            chunked_tokens.append(tok_pad)\n",
    "            chunk_attention_mask.append(attention_pad)\n",
    "            chunk_label_ids.append(label_pad)\n",
    "        input_ids_list.append(chunked_tokens)\n",
    "        attention_mask_list.append(chunk_attention_mask)\n",
    "        labels_list.append(chunk_label_ids)\n",
    "\n",
    "    \n",
    "    temp_ds = Dataset.from_dict({\"input_ids\": input_ids_list, \"labels\": labels_list, \"attention_mask\": attention_mask_list})\n",
    "    batched_dataset[dataset_name] = batched_dataset[dataset_name].cast(temp_ds.features)\n",
    "    batched_dataset[dataset_name]=concatenate_datasets([batched_dataset[dataset_name], temp_ds])\n",
    "    return sents_batch\n"
   ],
   "id": "d73f045c959ac137",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:41:56.966484Z",
     "start_time": "2024-06-19T13:38:13.783408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "inputid_dataset[\"train\"] = inputid_dataset[\"train\"].remove_columns('sentences')\n",
    "inputid_dataset[\"validation\"] = inputid_dataset[\"validation\"].remove_columns('sentences')\n",
    "inputid_dataset[\"test\"] = inputid_dataset[\"test\"].remove_columns('sentences')\n",
    "\n",
    "inputid_dataset[\"train\"].map(chunck_pad_map,batched=True, load_from_cache_file=False,fn_kwargs={\"dataset_name\": \"train\"})\n",
    "inputid_dataset[\"validation\"].map(chunck_pad_map,batched=True, load_from_cache_file=False,fn_kwargs={\"dataset_name\": \"validation\"})\n",
    "inputid_dataset[\"test\"].map(chunck_pad_map,batched=True, load_from_cache_file=False,fn_kwargs={\"dataset_name\": \"test\"})"
   ],
   "id": "74247212ae9bf166",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:41:56.978602Z",
     "start_time": "2024-06-19T13:41:56.970353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(inputid_dataset)\n",
    "print(batched_dataset)"
   ],
   "id": "80ee93b3285255ca",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:41:57.005211Z",
     "start_time": "2024-06-19T13:41:56.980369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batched_dataset['train'] = batched_dataset['train'].select(range(1, len(batched_dataset['train'])))\n",
    "batched_dataset['validation'] = batched_dataset['validation'].select(range(1, len(batched_dataset['validation'])))\n",
    "batched_dataset['test'] = batched_dataset['test'].select(range(1, len(batched_dataset['test'])))\n",
    "batched_dataset"
   ],
   "id": "9628957153ec7606",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:41:57.013349Z",
     "start_time": "2024-06-19T13:41:57.007530Z"
    }
   },
   "cell_type": "code",
   "source": "# inputid_dataset.save_to_disk(\"./../_data/datasets/batched/bert-mult-uncased\")",
   "id": "5cffd14c524ac3ff",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T13:43:16.719965Z",
     "start_time": "2024-06-19T13:43:16.220311Z"
    }
   },
   "cell_type": "code",
   "source": "batched_dataset.save_to_disk(\"./../_data/datasets/batched/bert-base-multilingual-uncased\")",
   "id": "ccc7e5fd138cd167",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "b7fcaf7938c6090c",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
